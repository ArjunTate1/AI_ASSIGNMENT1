{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **MIT Academy of Engineering (MIT AOE)**\n",
        "## **Department of Artificial Intelligence**\n",
        "### **Assignment No. 1 â€“ Boston Housing Dataset**\n",
        "\n",
        "####Problem Type: Regression\n",
        "\n",
        "Objective: Predict the Median value of owner-occupied homes ($1000s).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Name | Roll Number |\n",
        "|------|--------------|\n",
        "| Arjun Tate | 202401110061 |\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HOYJSniIXmFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. DATA EXPLORATION\n"
      ],
      "metadata": {
        "id": "yLbe2zZUay2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Load dataset from online source (Heart Disease Dataset - Cleveland)\n",
        "# The dataset has no header and uses '?' for missing values, which pandas detects as objects.\n",
        "# We explicitly set '?' as the na_values.\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
        "df = pd.read_csv(url, header=None, na_values='?')\n",
        "\n",
        "# Rename columns for clarity (based on UCI documentation)\n",
        "# The column index is used as the column name\n",
        "df.columns = [str(i) for i in range(df.shape[1])]\n",
        "df.rename(columns={'13': 'target'}, inplace=True) # Assuming the last column is the target\n",
        "\n",
        "print(\"Initial Dataset Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nInitial Dataset Info:\")\n",
        "df.info()\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df.describe().T)"
      ],
      "metadata": {
        "id": "4cwbH56Na9kU",
        "outputId": "2274387c-f539-4fee-f389-b25292e9be63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Dataset Head:\n",
            "      0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
            "0  63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0   \n",
            "1  67.0  1.0  4.0  160.0  286.0  0.0  2.0  108.0  1.0  1.5  2.0  3.0  3.0   \n",
            "2  67.0  1.0  4.0  120.0  229.0  0.0  2.0  129.0  1.0  2.6  2.0  2.0  7.0   \n",
            "3  37.0  1.0  3.0  130.0  250.0  0.0  0.0  187.0  0.0  3.5  3.0  0.0  3.0   \n",
            "4  41.0  0.0  2.0  130.0  204.0  0.0  2.0  172.0  0.0  1.4  1.0  0.0  3.0   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       2  \n",
            "2       1  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "Initial Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       303 non-null    float64\n",
            " 1   1       303 non-null    float64\n",
            " 2   2       303 non-null    float64\n",
            " 3   3       303 non-null    float64\n",
            " 4   4       303 non-null    float64\n",
            " 5   5       303 non-null    float64\n",
            " 6   6       303 non-null    float64\n",
            " 7   7       303 non-null    float64\n",
            " 8   8       303 non-null    float64\n",
            " 9   9       303 non-null    float64\n",
            " 10  10      303 non-null    float64\n",
            " 11  11      299 non-null    float64\n",
            " 12  12      301 non-null    float64\n",
            " 13  target  303 non-null    int64  \n",
            "dtypes: float64(13), int64(1)\n",
            "memory usage: 33.3 KB\n",
            "\n",
            "Summary Statistics:\n",
            "        count        mean        std    min    25%    50%    75%    max\n",
            "0       303.0   54.438944   9.038662   29.0   48.0   56.0   61.0   77.0\n",
            "1       303.0    0.679868   0.467299    0.0    0.0    1.0    1.0    1.0\n",
            "2       303.0    3.158416   0.960126    1.0    3.0    3.0    4.0    4.0\n",
            "3       303.0  131.689769  17.599748   94.0  120.0  130.0  140.0  200.0\n",
            "4       303.0  246.693069  51.776918  126.0  211.0  241.0  275.0  564.0\n",
            "5       303.0    0.148515   0.356198    0.0    0.0    0.0    0.0    1.0\n",
            "6       303.0    0.990099   0.994971    0.0    0.0    1.0    2.0    2.0\n",
            "7       303.0  149.607261  22.875003   71.0  133.5  153.0  166.0  202.0\n",
            "8       303.0    0.326733   0.469794    0.0    0.0    0.0    1.0    1.0\n",
            "9       303.0    1.039604   1.161075    0.0    0.0    0.8    1.6    6.2\n",
            "10      303.0    1.600660   0.616226    1.0    1.0    2.0    2.0    3.0\n",
            "11      299.0    0.672241   0.937438    0.0    0.0    0.0    1.0    3.0\n",
            "12      301.0    4.734219   1.939706    3.0    3.0    3.0    7.0    7.0\n",
            "target  303.0    0.937294   1.228536    0.0    0.0    0.0    2.0    4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Handle Missing Data"
      ],
      "metadata": {
        "id": "76k_fYXkbIAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Missing values before imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "\n",
        "\n",
        "if len(cat_cols) > 0:\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "BGcoFLmfbO3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encode Categorical Variables"
      ],
      "metadata": {
        "id": "rlJJJPkjbaN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_encode = ['2', '6', '10', '12']\n",
        "for col in cols_to_encode:\n",
        "    df[col] = df[col].astype('category')\n",
        "df_encoded = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)\n",
        "\n",
        "df_encoded.columns = df_encoded.columns.astype(str)\n",
        "\n",
        "print(\"\\nEncoded Dataset Head:\")\n",
        "print(df_encoded.head())\n",
        "print(f\"Encoded Dataset Shape: {df_encoded.shape}\")"
      ],
      "metadata": {
        "id": "ZfbLWyrDbfXU",
        "outputId": "1872f789-4b09-4b17-f18f-d25cd37ae743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Dataset Head:\n",
            "      0    1      3      4    5      7    8    9   11  target  2_2.0  2_3.0  \\\n",
            "0  63.0  1.0  145.0  233.0  1.0  150.0  0.0  2.3  0.0       0  False  False   \n",
            "1  67.0  1.0  160.0  286.0  0.0  108.0  1.0  1.5  3.0       2  False  False   \n",
            "2  67.0  1.0  120.0  229.0  0.0  129.0  1.0  2.6  2.0       1  False  False   \n",
            "3  37.0  1.0  130.0  250.0  0.0  187.0  0.0  3.5  0.0       0  False   True   \n",
            "4  41.0  0.0  130.0  204.0  0.0  172.0  0.0  1.4  0.0       0   True  False   \n",
            "\n",
            "   2_4.0  6_1.0  6_2.0  10_2.0  10_3.0  12_6.0  12_7.0  \n",
            "0  False  False   True   False    True    True   False  \n",
            "1   True  False   True    True   False   False   False  \n",
            "2   True  False   True    True   False   False    True  \n",
            "3  False  False  False   False    True   False   False  \n",
            "4  False  False   True   False   False   False   False  \n",
            "Encoded Dataset Shape: (303, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Feature Scaling"
      ],
      "metadata": {
        "id": "1nqajsUvbm64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = df_encoded.drop(columns=['target'], errors='ignore')\n",
        "y = df_encoded['target']\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the features\n",
        "scaled_features = scaler.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=X.columns)\n",
        "\n",
        "print(\"\\nScaled Dataset Head:\")\n",
        "print(scaled_df.head())"
      ],
      "metadata": {
        "id": "o65pLn2FbrYY",
        "outputId": "7bb748cb-0a67-40cb-86a7-fe553eba98a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scaled Dataset Head:\n",
            "          0         1         3         4         5         7         8  \\\n",
            "0  0.948726  0.686202  0.757525 -0.264900  2.394438  0.017197 -0.696631   \n",
            "1  1.392002  0.686202  1.611220  0.760415 -0.417635 -1.821905  1.435481   \n",
            "2  1.392002  0.686202 -0.665300 -0.342283 -0.417635 -0.902354  1.435481   \n",
            "3 -1.932564  0.686202 -0.096170  0.063974 -0.417635  1.637359 -0.696631   \n",
            "4 -1.489288 -1.457296 -0.096170 -0.825922 -0.417635  0.980537 -0.696631   \n",
            "\n",
            "          9        11     2_2.0     2_3.0     2_4.0     6_1.0     6_2.0  \\\n",
            "0  1.087338 -0.718306 -0.444554 -0.629534 -0.951662 -0.115663  1.023375   \n",
            "1  0.397182  2.487269 -0.444554 -0.629534  1.050793 -0.115663  1.023375   \n",
            "2  1.346147  1.418744 -0.444554 -0.629534  1.050793 -0.115663  1.023375   \n",
            "3  2.122573 -0.718306 -0.444554  1.588476 -0.951662 -0.115663 -0.977158   \n",
            "4  0.310912 -0.718306  2.249444 -0.629534 -0.951662 -0.115663  1.023375   \n",
            "\n",
            "     10_2.0    10_3.0    12_6.0    12_7.0  \n",
            "0 -0.926766  3.664502  3.979112 -0.793116  \n",
            "1  1.079021 -0.272888 -0.251312 -0.793116  \n",
            "2  1.079021 -0.272888 -0.251312  1.260850  \n",
            "3 -0.926766  3.664502 -0.251312 -0.793116  \n",
            "4 -0.926766 -0.272888 -0.251312 -0.793116  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. PCA (Dimensionality Reduction)"
      ],
      "metadata": {
        "id": "kARU9U6Nbtw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if scaled_df.isnull().values.any():\n",
        "    # If NaNs are found, we impute them using median *again* just for the PCA step\n",
        "    # Note: If this happens, it points to a problem in steps 2 or 3.\n",
        "    temp_imputer = SimpleImputer(strategy='median')\n",
        "    scaled_df_clean = pd.DataFrame(temp_imputer.fit_transform(scaled_df), columns=scaled_df.columns)\n",
        "else:\n",
        "    scaled_df_clean = scaled_df\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "# Use the cleaned version of the scaled data\n",
        "pca_features = pca.fit_transform(scaled_df_clean)\n",
        "\n",
        "# Convert PCA features back to DataFrame\n",
        "pca_df = pd.DataFrame(pca_features, columns=['PC1', 'PC2'])\n",
        "\n",
        "print(\"\\nPCA Transformed Dataset Head (Reduced to 2 Components):\")\n",
        "print(pca_df.head())"
      ],
      "metadata": {
        "id": "gzxuhMfNbw62",
        "outputId": "1a38adcf-7fb3-43a5-a875-5c166691ce1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PCA Transformed Dataset Head (Reduced to 2 Components):\n",
            "        PC1       PC2\n",
            "0  0.663443  2.237916\n",
            "1  3.487949  0.979003\n",
            "2  3.341976 -0.716217\n",
            "3 -1.683404  0.403007\n",
            "4 -2.505285 -0.374885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Feature Selection"
      ],
      "metadata": {
        "id": "QTXFcjJtcuyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Selection (SelectKBest) ---\n",
        "# We use the scaled, but not PCA-reduced, data for selection\n",
        "X_select = scaled_df # This DataFrame is the suspect for containing NaNs\n",
        "y_select = y\n",
        "\n",
        "# FIX: Impute any remaining NaNs in X_select before applying SelectKBest\n",
        "# Using median imputation again to clean up any NaNs that might have persisted\n",
        "nan_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Apply imputation to X_select. We ensure the output is a DataFrame for feature name retention.\n",
        "X_select_clean = pd.DataFrame(\n",
        "    nan_imputer.fit_transform(X_select),\n",
        "    columns=X_select.columns\n",
        ")\n",
        "\n",
        "\n",
        "# Select top 8 features as shown in the reference image\n",
        "selector = SelectKBest(score_func=f_classif, k=8)\n",
        "# Use the cleaned data (X_select_clean) for fitting\n",
        "selector.fit(X_select_clean, y_select)\n",
        "\n",
        "# Get the indices and names of the selected features\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_features = X_select_clean.columns[selected_indices]\n",
        "\n",
        "# Create the final selected dataset using the cleaned data\n",
        "X_new = X_select_clean[selected_features]\n",
        "\n",
        "print(\"\\nSelected Features (k=8):\")\n",
        "print(list(selected_features))\n",
        "print(\"\\nSelected Dataset Head:\")\n",
        "print(X_new.head())"
      ],
      "metadata": {
        "id": "IR3j5Ap0cwCc",
        "outputId": "9baf7587-2793-4a81-9afe-ee8bc9ce69d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected Features (k=8):\n",
            "['7', '8', '9', '11', '2_3.0', '2_4.0', '10_2.0', '12_7.0']\n",
            "\n",
            "Selected Dataset Head:\n",
            "          7         8         9        11     2_3.0     2_4.0    10_2.0  \\\n",
            "0  0.017197 -0.696631  1.087338 -0.718306 -0.629534 -0.951662 -0.926766   \n",
            "1 -1.821905  1.435481  0.397182  2.487269 -0.629534  1.050793  1.079021   \n",
            "2 -0.902354  1.435481  1.346147  1.418744 -0.629534  1.050793  1.079021   \n",
            "3  1.637359 -0.696631  2.122573 -0.718306  1.588476 -0.951662 -0.926766   \n",
            "4  0.980537 -0.696631  0.310912 -0.718306 -0.629534 -0.951662 -0.926766   \n",
            "\n",
            "     12_7.0  \n",
            "0 -0.793116  \n",
            "1 -0.793116  \n",
            "2  1.260850  \n",
            "3 -0.793116  \n",
            "4 -0.793116  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Summary of Transformations"
      ],
      "metadata": {
        "id": "mJYWWZGvdIS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We use the scaled, but not PCA-reduced, data for selection\n",
        "X_select = scaled_df # This DataFrame is the suspect for containing NaNs\n",
        "y_select = y\n",
        "\n",
        "# FIX: Impute any remaining NaNs in X_select before applying SelectKBest\n",
        "# Using median imputation again to clean up any NaNs that might have persisted\n",
        "nan_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Apply imputation to X_select. We ensure the output is a DataFrame for feature name retention.\n",
        "X_select_clean = pd.DataFrame(\n",
        "    nan_imputer.fit_transform(X_select),\n",
        "    columns=X_select.columns\n",
        ")\n",
        "\n",
        "\n",
        "# Select top 8 features as shown in the reference image\n",
        "selector = SelectKBest(score_func=f_classif, k=8)\n",
        "# Use the cleaned data (X_select_clean) for fitting\n",
        "selector.fit(X_select_clean, y_select)\n",
        "\n",
        "# Get the indices and names of the selected features\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_features = X_select_clean.columns[selected_indices]\n",
        "\n",
        "# Create the final selected dataset using the cleaned data\n",
        "X_new = X_select_clean[selected_features]\n",
        "\n",
        "print(\"\\nSelected Features (k=8):\")\n",
        "print(list(selected_features))\n",
        "print(\"\\nSelected Dataset Head:\")\n",
        "print(X_new.head())"
      ],
      "metadata": {
        "id": "jBENNkvIdJSV",
        "outputId": "c0afaa0d-72d2-42da-c60c-19b1d3e488f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected Features (k=8):\n",
            "['7', '8', '9', '11', '2_3.0', '2_4.0', '10_2.0', '12_7.0']\n",
            "\n",
            "Selected Dataset Head:\n",
            "          7         8         9        11     2_3.0     2_4.0    10_2.0  \\\n",
            "0  0.017197 -0.696631  1.087338 -0.718306 -0.629534 -0.951662 -0.926766   \n",
            "1 -1.821905  1.435481  0.397182  2.487269 -0.629534  1.050793  1.079021   \n",
            "2 -0.902354  1.435481  1.346147  1.418744 -0.629534  1.050793  1.079021   \n",
            "3  1.637359 -0.696631  2.122573 -0.718306  1.588476 -0.951662 -0.926766   \n",
            "4  0.980537 -0.696631  0.310912 -0.718306 -0.629534 -0.951662 -0.926766   \n",
            "\n",
            "     12_7.0  \n",
            "0 -0.793116  \n",
            "1 -0.793116  \n",
            "2  1.260850  \n",
            "3 -0.793116  \n",
            "4 -0.793116  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "The feature engineering process successfully transformed the raw Heart Disease dataset into a robust and model-ready format. This transformation began with cleaning the data through imputation to handle missing values, followed by One-Hot Encoding to convert categorical features into a quantitative, machine-readable format. Subsequently, Standardization was applied, which is critical for distance-based models, by scaling all numeric features to a common range. Finally, the data quality was further refined and optimized by applying PCA for dimensionality reduction and SelectKBest for retaining only the top 8 most statistically predictive features. This comprehensive pre-processing ensures the final dataset is free of missing values, consistently scaled, and highly informative, thereby enhancing the interpretability and predictive accuracy of any subsequent machine learning model."
      ],
      "metadata": {
        "id": "GUoRCIDHdthW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}